# -*- coding: utf-8 -*-
"""Creatingfaces.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1omrfNXPWT_0AybvUlNZVXHPOepbgsrZf
"""

import numpy as np
import cv2
import os
import tensorflow as tf
from tensorflow.keras.layers import Dense,Flatten,Conv2D,Conv2DTranspose,Reshape,LeakyReLU,Dropout,BatchNormalization,Activation,UpSampling2D,MaxPooling2D
from tensorflow.keras.models import Sequential
import time
from tensorflow.keras.optimizers import SGD
from tensorflow.keras.models import load_model
import matplotlib.pyplot as plt

! pip install -q kaggle
from google.colab import files
files.upload()
! mkdir ~/.kaggle
! cp kaggle.json ~/.kaggle/
! chmod 600 ~/.kaggle/kaggle.json

! kaggle datasets download -d greg115/celebrities-100k

!unzip /content/celebrities-100k.zip

folder = []
def load_Image(folder):
    images = []
    for j in range(len(folder)):
        for filename in os.listdir(folder[j]):
            img = cv2.imread(os.path.join(folder[j],filename))
            img = cv2.resize(img,(64,64))
            if img is not None and len(images)<15000:
                images.append(img)
    return np.asarray(images)

test = ['/content/100k/100k']
x = load_Image(test)

x = (x - 127.5) / 127.5

dis_learning_rate = 0.0004
gen_learning_rate = 0.0004
dis_momentum = 0.9
gen_momentum = 0.9
dis_nesterov = True
gen_nesterov = True

dis_optimizer = SGD(lr=dis_learning_rate, momentum=dis_momentum,
nesterov=dis_nesterov)
gen_optimizer = SGD(lr=gen_learning_rate, momentum=gen_momentum,
nesterov=gen_nesterov)

gen_model = Sequential()
gen_model.add(Dense(units=2048))
gen_model.add(Activation('tanh'))
gen_model.add(Dense(256 * 8 * 8))
gen_model.add(BatchNormalization())
gen_model.add(LeakyReLU(alpha=0.2))
gen_model.add(Reshape((8, 8, 256), input_shape=(256 * 8 * 8,)))
gen_model.add(UpSampling2D(size=(2, 2)))
gen_model.add(Conv2D(128, (5, 5), padding='same'))
gen_model.add(LeakyReLU(alpha=0.2))
gen_model.add(UpSampling2D(size=(2, 2)))
gen_model.add(Conv2D(64, (5, 5), padding='same'))
gen_model.add(LeakyReLU(alpha=0.2))
gen_model.add(UpSampling2D(size=(2, 2)))
gen_model.add(Conv2D(3, (5, 5), padding='same'))
gen_model.add(LeakyReLU(alpha=0.2))

dis_model = Sequential()
dis_model.add(
Conv2D(128, (5, 5),
padding='same',
input_shape=(64, 64, 3))
)
dis_model.add(LeakyReLU(alpha=0.2))
dis_model.add(MaxPooling2D(pool_size=(2, 2)))
dis_model.add(Conv2D(256, (3, 3)))
dis_model.add(LeakyReLU(alpha=0.2))
dis_model.add(MaxPooling2D(pool_size=(2, 2)))
dis_model.add(Conv2D(512, (3, 3)))
dis_model.add(LeakyReLU(alpha=0.2))
dis_model.add(MaxPooling2D(pool_size=(2, 2)))
dis_model.add(Flatten())
dis_model.add(Dense(1024))
dis_model.add(LeakyReLU(alpha=0.2))
dis_model.add(Dense(1))
dis_model.add(Activation('sigmoid'))
dis_model.compile(loss='binary_crossentropy',
optimizer=dis_optimizer)

GAN = Sequential([gen_model,dis_model])
dis_model.trainable = False
GAN.compile(loss='binary_crossentropy',
optimizer=gen_optimizer)

batch_size = 256
my_data = x
dataset = tf.data.Dataset.from_tensor_slices(my_data).shuffle(buffer_size =1000)
dataset = dataset.batch(batch_size,drop_remainder=True).prefetch(1)

epochs=1000

generator, discriminator = GAN.layers

coding_size=100
for epoch in range(epochs):
    print(f"Currently on epoch {epoch+1}")
    i=0
    for X_batch in dataset:
        i=i+1
        if i%10 ==0:
            print(f"\t currently on batch number {i} of {len(my_data)//batch_size}")
        noise = tf.random.normal(shape=[batch_size,coding_size])
        gen_images = generator(noise)
        x_fake_vs_real = tf.concat([gen_images, tf.dtypes.cast(X_batch,tf.float32)],axis=0)
        y1 = tf.constant([[0.0]]*batch_size+[[1.0]]*batch_size)
        y1 = np.asarray(y1)
        discriminator.trainable = True
        d_loss = discriminator.train_on_batch(x_fake_vs_real,y1)

        #Train Generator
        noise = tf.random.normal(shape=[batch_size,coding_size])
        y2 = tf.constant([[1.0]]*batch_size)
        g_loss = GAN.train_on_batch(noise,y2)
    print(f"\t Generator loss: {g_loss} and Discriminator loss: {d_loss}")
    noise = tf.random.normal(shape=[10,coding_size])
    images = generator(noise)
    plt.figure()
    f, axarr = plt.subplots(4,1,figsize=(16,16))
    axarr[0].imshow(cv2.cvtColor((images[0]).numpy(), cv2.COLOR_BGR2RGB)) 
    axarr[1].imshow(cv2.cvtColor((images[1]).numpy(), cv2.COLOR_BGR2RGB))   
    axarr[2].imshow(cv2.cvtColor((images[2]).numpy(), cv2.COLOR_BGR2RGB))  
    axarr[3].imshow(cv2.cvtColor((images[3]).numpy(), cv2.COLOR_BGR2RGB))   
 
 
    plt.show()

noise = np.random.normal(0,1 ,size=(10,coding_size))
gen_images = gen_model.predict_on_batch(noise)
plt.imshow(((images[0])).numpy())

generator.save('/content')